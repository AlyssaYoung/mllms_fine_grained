[NaNGuard] RoPE.out.cos: dtype=torch.float16 min=-1.0000e+00 max=1.0000e+00 mean=5.5603e-02 std=7.0654e-01
[NaNGuard] RoPE.out.sin: dtype=torch.float16 min=-1.0000e+00 max=1.0000e+00 mean=1.1810e-01 std=6.9531e-01
[NaNGuard] position_ids: min=0, max=9265, len=9266
[NaNGuard] Layer0.input_ln.out: dtype=torch.float16 min=-4.1211e+00 max=4.9609e+00 mean=3.3855e-04 std=6.2988e-02
[NaNGuard] Layer0.attn.out: dtype=torch.float16 min=-4.1284e-01 max=8.9746e-01 mean=9.8109e-05 std=1.2909e-02
[NaNGuard] Layer0.post_attn_ln.out: dtype=torch.float16 min=-2.2852e+00 max=1.3672e+00 mean=-1.9658e-04 std=5.8868e-02
[NaNGuard] Layer0.mlp.out: dtype=torch.float16 min=-1.0566e+00 max=3.6230e+00 mean=2.0659e-04 std=1.0384e-02
[NaNGuard] Layer1.input_ln.out: dtype=torch.float16 min=-3.3418e+00 max=8.9297e+00 mean=-1.3542e-03 std=1.1737e-01
[NaNGuard] Layer1.attn.out: dtype=torch.float16 min=-2.3359e+00 max=6.0986e-01 mean=5.0068e-05 std=2.7481e-02
[NaNGuard] Layer1.post_attn_ln.out: dtype=torch.float16 min=-1.0498e+00 max=1.8105e+00 mean=-3.0303e-04 std=9.9609e-02
[NaNGuard] Layer1.mlp.out: dtype=torch.float16 min=-1.4990e+03 max=2.5260e+03 mean=6.3181e-05 std=5.9668e-01
[NaNGuard] Layer2.input_ln.out: dtype=torch.float16 min=-5.4258e+00 max=7.9922e+00 mean=-1.5278e-03 std=1.8713e-01
[NaNGuard] Layer2.attn.out: dtype=torch.float16 min=-1.0020e+00 max=1.6973e+00 mean=4.9686e-04 std=8.8440e-02
[NaNGuard] Layer2.post_attn_ln.out: dtype=torch.float16 min=-4.1875e+00 max=5.6992e+00 mean=-4.3988e-04 std=1.2964e-01
[NaNGuard] Layer2.mlp.out: dtype=torch.float16 min=-2.7969e+00 max=4.4180e+00 mean=-7.2956e-05 std=3.3325e-02
[NaNGuard] Layer3.input_ln.out: dtype=torch.float16 min=-6.2148e+00 max=7.5195e+00 mean=-2.2984e-03 std=2.9590e-01
[NaNGuard] Layer3.attn.out: dtype=torch.float16 min=-1.3408e+00 max=4.4609e+00 mean=1.9341e-03 std=1.2061e-01
[NaNGuard] Layer3.post_attn_ln.out: dtype=torch.float16 min=-3.6816e+00 max=2.8750e+00 mean=-1.3459e-04 std=1.6345e-01
[NaNGuard] Layer3.mlp.out: dtype=torch.float16 min=-3.4473e+00 max=8.6865e-01 mean=1.4067e-05 std=5.9021e-02
[NaNGuard] Layer4.input_ln.out: dtype=torch.float16 min=-6.7383e+00 max=8.0312e+00 mean=-8.0204e-04 std=2.7905e-01
[NaNGuard] Layer4.attn.out: dtype=torch.float16 min=-6.9238e-01 max=2.7734e+00 mean=3.4547e-04 std=1.1554e-01
[NaNGuard] Layer4.post_attn_ln.out: dtype=torch.float16 min=-6.2969e+00 max=7.5430e+00 mean=-2.7919e-04 std=1.7944e-01
[NaNGuard] Layer4.mlp.out: dtype=torch.float16 min=-6.3477e+00 max=1.0430e+01 mean=1.8239e-05 std=8.5205e-02
[NaNGuard] Layer5.input_ln.out: dtype=torch.float16 min=-6.9844e+00 max=9.0312e+00 mean=5.4419e-05 std=2.8662e-01
[NaNGuard] Layer5.attn.out: dtype=torch.float16 min=-8.0469e-01 max=4.1484e+00 mean=2.2328e-04 std=1.2152e-01
[NaNGuard] Layer5.post_attn_ln.out: dtype=torch.float16 min=-5.6484e+00 max=3.1582e+00 mean=-1.3673e-04 std=1.9006e-01
[NaNGuard] Layer5.mlp.out: dtype=torch.float16 min=-3.8574e+00 max=8.3936e-01 mean=3.2127e-05 std=9.5520e-02
[NaNGuard] Layer6.input_ln.out: dtype=torch.float16 min=-8.5469e+00 max=1.0391e+01 mean=4.1676e-04 std=3.5205e-01
[NaNGuard] Layer6.attn.out: dtype=torch.float16 min=-1.8193e+00 max=5.9688e+00 mean=4.2572e-03 std=1.3745e-01
[NaNGuard] Layer6.post_attn_ln.out: dtype=torch.float16 min=-5.7266e+00 max=3.1445e+00 mean=1.3037e-03 std=2.0386e-01
[NaNGuard] Layer6.mlp.out: dtype=torch.float16 min=-6.9531e+00 max=1.2705e+00 mean=-2.1362e-03 std=1.1078e-01
[NaNGuard] Layer7.input_ln.out: dtype=torch.float16 min=-1.2469e+01 max=1.1281e+01 mean=2.2316e-03 std=3.6816e-01
[NaNGuard] Layer7.attn.out: dtype=torch.float16 min=-2.3164e+00 max=4.9492e+00 mean=6.8951e-04 std=1.3440e-01
[NaNGuard] Layer7.post_attn_ln.out: dtype=torch.float16 min=-6.5586e+00 max=3.6133e+00 mean=1.0214e-03 std=2.1643e-01
[NaNGuard] Layer7.mlp.out: dtype=torch.float16 min=-4.7227e+00 max=7.4707e-01 mean=-6.7663e-04 std=1.1816e-01
[NaNGuard] Layer8.input_ln.out: dtype=torch.float16 min=-1.8531e+01 max=1.2367e+01 mean=3.0079e-03 std=3.6719e-01
[NaNGuard] Layer8.attn.out: dtype=torch.float16 min=-1.1982e+00 max=6.0273e+00 mean=2.7962e-03 std=1.4722e-01
[NaNGuard] Layer8.post_attn_ln.out: dtype=torch.float16 min=-8.5156e+00 max=3.6934e+00 mean=1.8768e-03 std=2.2095e-01
[NaNGuard] Layer8.mlp.out: dtype=torch.float16 min=-6.3906e+00 max=2.5273e+00 mean=-7.2193e-04 std=1.3489e-01
[NaNGuard] Layer9.input_ln.out: dtype=torch.float16 min=-2.3719e+01 max=1.2891e+01 mean=4.2992e-03 std=3.7256e-01
[NaNGuard] Layer9.attn.out: dtype=torch.float16 min=-1.5010e+00 max=5.2734e+00 mean=-9.2149e-05 std=1.5356e-01
[NaNGuard] Layer9.post_attn_ln.out: dtype=torch.float16 min=-1.1180e+01 max=4.0430e+00 mean=1.5440e-03 std=2.2681e-01
[NaNGuard] Layer9.mlp.out: dtype=torch.float16 min=-4.5391e+00 max=3.3672e+00 mean=-1.5726e-03 std=1.3464e-01
[NaNGuard] Layer10.input_ln.out: dtype=torch.float16 min=-3.0578e+01 max=1.4148e+01 mean=3.8834e-03 std=3.7720e-01
[NaNGuard] Layer10.attn.out: dtype=torch.float16 min=-2.1992e+00 max=5.6797e+00 mean=3.2425e-04 std=1.5479e-01
[NaNGuard] Layer10.post_attn_ln.out: dtype=torch.float16 min=-6.2148e+00 max=4.4688e+00 mean=1.1377e-03 std=2.2974e-01
[NaNGuard] Layer10.mlp.out: dtype=torch.float16 min=-3.6992e+00 max=3.8203e+00 mean=-7.2336e-04 std=1.3696e-01
[NaNGuard] Layer11.input_ln.out: dtype=torch.float16 min=-2.4281e+01 max=1.5219e+01 mean=4.2572e-03 std=4.1260e-01
[NaNGuard] Layer11.attn.out: dtype=torch.float16 min=-4.7070e+00 max=4.1445e+00 mean=-9.0837e-05 std=1.6357e-01
[NaNGuard] Layer11.post_attn_ln.out: dtype=torch.float16 min=-6.1250e+00 max=4.4961e+00 mean=8.2254e-04 std=2.3706e-01
[NaNGuard] Layer11.mlp.out: dtype=torch.float16 min=-4.5000e+00 max=2.9336e+00 mean=-6.9189e-04 std=1.3269e-01
[NaNGuard] Layer12.input_ln.out: dtype=torch.float16 min=-2.4703e+01 max=1.4188e+01 mean=3.5591e-03 std=4.1235e-01
[NaNGuard] Layer12.attn.out: dtype=torch.float16 min=-3.1953e+00 max=3.3496e+00 mean=1.3752e-03 std=1.7578e-01
[NaNGuard] Layer12.post_attn_ln.out: dtype=torch.float16 min=-7.2734e+00 max=4.0156e+00 mean=1.1482e-03 std=2.4316e-01
[NaNGuard] Layer12.mlp.out: dtype=torch.float16 min=-4.0273e+00 max=1.7207e+00 mean=9.0075e-04 std=1.3220e-01
[NaNGuard] Layer13.input_ln.out: dtype=torch.float16 min=-2.1031e+01 max=1.5141e+01 mean=5.1155e-03 std=4.1577e-01
[NaNGuard] Layer13.attn.out: dtype=torch.float16 min=-4.4609e+00 max=3.8711e+00 mean=1.7948e-03 std=1.7444e-01
[NaNGuard] Layer13.post_attn_ln.out: dtype=torch.float16 min=-8.7734e+00 max=4.3086e+00 mean=2.2945e-03 std=2.4902e-01
[NaNGuard] Layer13.mlp.out: dtype=torch.float16 min=-4.1133e+00 max=4.1016e+00 mean=-1.5478e-03 std=1.4111e-01
[NaNGuard] Layer14.input_ln.out: dtype=torch.float16 min=-2.1750e+01 max=1.4844e+01 mean=5.1842e-03 std=4.2920e-01
[NaNGuard] Layer14.attn.out: dtype=torch.float16 min=-5.1797e+00 max=4.6914e+00 mean=2.0981e-03 std=1.9751e-01
[NaNGuard] Layer14.post_attn_ln.out: dtype=torch.float16 min=-7.5039e+00 max=4.5508e+00 mean=2.4281e-03 std=2.6001e-01
[NaNGuard] Layer14.mlp.out: dtype=torch.float16 min=-3.0625e+00 max=2.1094e+00 mean=-1.3571e-03 std=1.3794e-01
[NaNGuard] Layer15.input_ln.out: dtype=torch.float16 min=-2.0234e+01 max=1.6641e+01 mean=5.6801e-03 std=4.2627e-01
[NaNGuard] Layer15.attn.out: dtype=torch.float16 min=-6.7617e+00 max=2.8242e+00 mean=-1.4305e-03 std=1.9641e-01
[NaNGuard] Layer15.post_attn_ln.out: dtype=torch.float16 min=-8.8359e+00 max=5.1406e+00 mean=1.4191e-03 std=2.6929e-01
[NaNGuard] Layer15.mlp.out: dtype=torch.float16 min=-3.4238e+00 max=8.1484e+00 mean=9.1696e-04 std=1.5601e-01
[NaNGuard] Layer16.input_ln.out: dtype=torch.float16 min=-1.9500e+01 max=1.5406e+01 mean=5.2185e-03 std=4.3652e-01
[NaNGuard] Layer16.attn.out: dtype=torch.float16 min=-3.1797e+00 max=4.1172e+00 mean=9.7513e-04 std=1.8555e-01
[NaNGuard] Layer16.post_attn_ln.out: dtype=torch.float16 min=-8.6172e+00 max=9.3672e+00 mean=1.9817e-03 std=2.8516e-01
[NaNGuard] Layer16.mlp.out: dtype=torch.float16 min=-5.4766e+00 max=6.7188e+00 mean=1.4324e-03 std=1.6321e-01
[NaNGuard] Layer17.input_ln.out: dtype=torch.float16 min=-1.4703e+01 max=1.5438e+01 mean=6.4430e-03 std=4.4482e-01
[NaNGuard] Layer17.attn.out: dtype=torch.float16 min=-1.0328e+01 max=5.4883e+00 mean=-1.8749e-03 std=2.1118e-01
[NaNGuard] Layer17.post_attn_ln.out: dtype=torch.float16 min=-7.7305e+00 max=4.9570e+00 mean=1.6499e-03 std=2.9688e-01
[NaNGuard] Layer17.mlp.out: dtype=torch.float16 min=-2.9551e+00 max=4.9805e+00 mean=-1.5497e-03 std=1.5198e-01
[NaNGuard] Layer18.input_ln.out: dtype=torch.float16 min=-1.6875e+01 max=1.5531e+01 mean=4.3869e-03 std=4.6240e-01
[NaNGuard] Layer18.attn.out: dtype=torch.float16 min=-1.0266e+01 max=3.2637e+00 mean=8.1253e-04 std=2.2546e-01
[NaNGuard] Layer18.post_attn_ln.out: dtype=torch.float16 min=-7.5273e+00 max=5.1719e+00 mean=1.4801e-03 std=3.0737e-01
[NaNGuard] Layer18.mlp.out: dtype=torch.float16 min=-1.1172e+01 max=5.6641e+00 mean=-1.9722e-03 std=1.9495e-01
[NaNGuard] Layer19.input_ln.out: dtype=torch.float16 min=-1.3703e+01 max=1.5117e+01 mean=3.8128e-03 std=4.4800e-01
[NaNGuard] Layer19.attn.out: dtype=torch.float16 min=-8.7578e+00 max=6.3438e+00 mean=6.7091e-04 std=2.4622e-01
[NaNGuard] Layer19.post_attn_ln.out: dtype=torch.float16 min=-7.7578e+00 max=9.0000e+00 mean=1.4467e-03 std=3.1006e-01
[NaNGuard] Layer19.mlp.out: dtype=torch.float16 min=-4.9531e+00 max=9.3438e+00 mean=7.4267e-05 std=1.7957e-01
[NaNGuard] Layer20.input_ln.out: dtype=torch.float16 min=-1.2867e+01 max=1.5211e+01 mean=4.3869e-03 std=4.4604e-01
[NaNGuard] Layer20.attn.out: dtype=torch.float16 min=-5.5117e+00 max=7.9453e+00 mean=-6.8808e-04 std=2.1667e-01
[NaNGuard] Layer20.post_attn_ln.out: dtype=torch.float16 min=-6.8789e+00 max=5.3359e+00 mean=7.8583e-04 std=3.1763e-01
[NaNGuard] Layer20.mlp.out: dtype=torch.float16 min=-7.8242e+00 max=1.1219e+01 mean=1.6317e-03 std=2.2046e-01
[NaNGuard] Layer21.input_ln.out: dtype=torch.float16 min=-1.4008e+01 max=1.5156e+01 mean=4.1695e-03 std=4.7705e-01
[NaNGuard] Layer21.attn.out: dtype=torch.float16 min=-2.3164e+00 max=9.6797e+00 mean=-8.7309e-04 std=2.2656e-01
[NaNGuard] Layer21.post_attn_ln.out: dtype=torch.float16 min=-7.1250e+00 max=5.0664e+00 mean=4.2653e-04 std=3.3203e-01
[NaNGuard] Layer21.mlp.out: dtype=torch.float16 min=-6.3984e+00 max=2.2285e+00 mean=-7.0333e-04 std=2.1167e-01
[NaNGuard] Layer22.input_ln.out: dtype=torch.float16 min=-1.4922e+01 max=1.5750e+01 mean=3.4981e-03 std=4.8804e-01
[NaNGuard] Layer22.attn.out: dtype=torch.float16 min=-1.6031e+01 max=1.1789e+01 mean=-2.9850e-03 std=2.9321e-01
[NaNGuard] Layer22.post_attn_ln.out: dtype=torch.float16 min=-7.1367e+00 max=4.9883e+00 mean=-6.2990e-04 std=3.3252e-01
[NaNGuard] Layer22.mlp.out: dtype=torch.float16 min=-2.2246e+00 max=2.2051e+00 mean=-3.9935e-04 std=2.0361e-01
[NaNGuard] Layer23.input_ln.out: dtype=torch.float16 min=-1.3695e+01 max=1.4203e+01 mean=1.6651e-03 std=4.9316e-01
[NaNGuard] Layer23.attn.out: dtype=torch.float16 min=-6.3047e+00 max=6.2266e+00 mean=-1.4603e-04 std=2.8467e-01
[NaNGuard] Layer23.post_attn_ln.out: dtype=torch.float16 min=-6.7891e+00 max=4.8789e+00 mean=-8.9979e-04 std=3.4277e-01
[NaNGuard] Layer23.mlp.out: dtype=torch.float16 min=-1.2672e+01 max=4.2422e+00 mean=9.5415e-04 std=2.4304e-01
[NaNGuard] Layer24.input_ln.out: dtype=torch.float16 min=-1.4820e+01 max=1.5820e+01 mean=3.1719e-03 std=4.9170e-01
[NaNGuard] Layer24.attn.out: dtype=torch.float16 min=-1.3000e+01 max=1.7875e+01 mean=-5.5015e-05 std=2.8271e-01
[NaNGuard] Layer24.post_attn_ln.out: dtype=torch.float16 min=-7.2383e+00 max=4.6367e+00 mean=-4.4894e-04 std=3.3984e-01
[NaNGuard] Layer24.mlp.out: dtype=torch.float16 min=-3.0820e+00 max=5.6602e+00 mean=8.4448e-04 std=2.3755e-01
[NaNGuard] Layer25.input_ln.out: dtype=torch.float16 min=-1.2820e+01 max=1.4281e+01 mean=2.5578e-03 std=5.0732e-01
[NaNGuard] Layer25.attn.out: dtype=torch.float16 min=-5.8672e+00 max=3.5137e+00 mean=-5.1308e-04 std=2.4280e-01
[NaNGuard] Layer25.post_attn_ln.out: dtype=torch.float16 min=-7.4609e+00 max=5.5508e+00 mean=-2.7609e-04 std=3.5522e-01
[NaNGuard] Layer25.mlp.out: dtype=torch.float16 min=-1.0289e+01 max=5.9766e+00 mean=1.2074e-03 std=2.6611e-01
[NaNGuard] Layer26.input_ln.out: dtype=torch.float16 min=-1.4445e+01 max=1.6422e+01 mean=3.2616e-03 std=5.0684e-01
[NaNGuard] Layer26.attn.out: dtype=torch.float16 min=-2.5332e+00 max=2.0094e+01 mean=-6.4373e-06 std=2.7368e-01
[NaNGuard] Layer26.post_attn_ln.out: dtype=torch.float16 min=-8.2500e+00 max=6.1406e+00 mean=-2.4223e-04 std=3.6182e-01
[NaNGuard] Layer26.mlp.out: dtype=torch.float16 min=-2.8809e+00 max=3.3867e+00 mean=-3.5739e-04 std=2.2974e-01
[NaNGuard] Layer27.input_ln.out: dtype=torch.float16 min=-1.1445e+01 max=1.5500e+01 mean=1.9779e-03 std=4.9658e-01
[NaNGuard] Layer27.attn.out: dtype=torch.float16 min=-1.0500e+01 max=5.6914e+00 mean=5.1212e-04 std=2.8662e-01
[NaNGuard] Layer27.post_attn_ln.out: dtype=torch.float16 min=-9.0391e+00 max=7.3984e+00 mean=2.5392e-05 std=3.7256e-01
[NaNGuard] Layer27.mlp.out: dtype=torch.float16 min=-3.3125e+00 max=6.9336e+00 mean=7.6580e-04 std=2.7368e-01
[NaNGuard] Layer28.input_ln.out: dtype=torch.float16 min=-1.1281e+01 max=1.4273e+01 mean=2.6779e-03 std=5.0195e-01
[NaNGuard] Layer28.attn.out: dtype=torch.float16 min=-7.8945e+00 max=8.2500e+00 mean=-4.7646e-03 std=2.9224e-01
[NaNGuard] Layer28.post_attn_ln.out: dtype=torch.float16 min=-9.7188e+00 max=8.1484e+00 mean=-9.9373e-04 std=3.8306e-01
[NaNGuard] Layer28.mlp.out: dtype=torch.float16 min=-7.3984e+00 max=1.0180e+01 mean=-2.9545e-03 std=3.3691e-01
[NaNGuard] Layer29.input_ln.out: dtype=torch.float16 min=-1.2312e+01 max=1.3734e+01 mean=4.5657e-04 std=4.8975e-01
[NaNGuard] Layer29.attn.out: dtype=torch.float16 min=-3.3066e+00 max=1.3828e+01 mean=-5.2261e-03 std=3.2959e-01
[NaNGuard] Layer29.post_attn_ln.out: dtype=torch.float16 min=-1.0523e+01 max=9.0156e+00 mean=-3.5095e-03 std=3.9014e-01
[NaNGuard] Layer29.mlp.out: dtype=torch.float16 min=-3.6475e+02 max=1.8575e+02 mean=-2.0099e-04 std=5.9961e-01
[NaNGuard] Layer30.input_ln.out: dtype=torch.float16 min=-1.0875e+01 max=1.2797e+01 mean=-2.3327e-03 std=4.9243e-01
[NaNGuard] Layer30.attn.out: dtype=torch.float16 min=-1.2781e+01 max=1.4156e+01 mean=4.9248e-03 std=4.0405e-01
[NaNGuard] Layer30.post_attn_ln.out: dtype=torch.float16 min=-1.1633e+01 max=1.6609e+01 mean=-2.0180e-03 std=3.9600e-01
  0%|                                                                                                              | 0/191 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/pinci/code/mllms_fine_grained/main.py", line 412, in <module>
    response = multiple_choices_inference(
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/pinci/code/mllms_fine_grained/main.py", line 245, in multiple_choices_inference
    output_question = model(
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pinci/code/mllms_fine_grained/llava/model/language_model/llava_llama.py", line 109, in forward
    return super().forward(
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
    outputs = self.model(
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 692, in forward
    hidden_states = self.mlp(hidden_states)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/mllms_fine/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1581, in _call_impl
    hook_result = hook(self, args, result)
  File "/home/pinci/code/mllms_fine_grained/main.py", line 100, in _hook
    _finite(_tag, out); _stats(_tag, out)
  File "/home/pinci/code/mllms_fine_grained/main.py", line 28, in _finite
    raise RuntimeError(f"[NaNGuard] {name} has NaN/Inf. shape={tuple(t.shape)}")
RuntimeError: [NaNGuard] Layer30.mlp.out has NaN/Inf. shape=(1, 9266, 4096)